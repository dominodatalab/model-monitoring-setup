{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Model Training\n",
    "\n",
    "Quick example training a Random Forest classifier on synthetic data for model monitoring demo. Run this notebook and then follow the steps in the project README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up MLflow experiment with unique name\n",
    "starting_domino_user = os.environ.get(\"DOMINO_STARTING_USERNAME\", \"default_user\")\n",
    "experiment_name = f\"model_monitoring_example_{starting_domino_user}\"\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# 10 features named 1_feature, 2_feature, etc.\n",
    "feature_names = [f'{i}_feature' for i in range(1, 11)]\n",
    "X = np.random.randn(n_samples, 10)\n",
    "\n",
    "# Create target with 4 classes\n",
    "# Add some signal to make classification meaningful\n",
    "signal = X[:, 0] + X[:, 1] * 0.5 + X[:, 2] * 0.3\n",
    "y = np.digitize(signal, bins=np.percentile(signal, [25, 50, 75]))\n",
    "class_names = [f'class_{i}' for i in range(1, 5)]\n",
    "\n",
    "# Create DataFrames\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = [class_names[i] for i in y]\n",
    "\n",
    "print(f\"MLflow experiment: {experiment_name}\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[feature_names], df['target'], \n",
    "    test_size=0.3, random_state=42, stratify=df['target']\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with MLflow tracking\n",
    "with mlflow.start_run() as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"n_features\", len(feature_names))\n",
    "    mlflow.log_param(\"n_classes\", len(class_names))\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate and log metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log model with input example and signature\n",
    "    input_example = X_train.iloc[:5]  # Use first 5 rows as example\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"random_forest_model\",\n",
    "        input_example=input_example\n",
    "    )\n",
    "    \n",
    "    # Store run_id for later use\n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"\\nMLflow run logged to experiment: {experiment_name}\")\n",
    "    print(f\"Run ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and data\n",
    "joblib.dump(model, 'notebook-model.pkl')\n",
    "\n",
    "# Save training data for monitoring baseline\n",
    "train_df = X_train.copy()\n",
    "train_df['target'] = y_train\n",
    "train_df.to_csv('/mnt/artifacts/training_data.csv', index=False)\n",
    "\n",
    "# Save test data for predictions\n",
    "test_df = X_test.copy()\n",
    "test_df['target'] = y_test\n",
    "test_df.to_csv('/mnt/artifacts/test_data.csv', index=False)\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"- model.pkl\")\n",
    "print(\"- /mnt/artifacts/training_data.csv\")\n",
    "print(\"- /mnt/artifacts/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test probability prediction\n",
    "sample = X_test.iloc[:1]\n",
    "pred_class = model.predict(sample)[0]\n",
    "pred_proba = model.predict_proba(sample)[0]\n",
    "\n",
    "print(f\"Sample prediction: {pred_class}\")\n",
    "print(f\"Class probabilities: {dict(zip(model.classes_, pred_proba))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export command for model deployment\n",
    "print(f\"✅ MLflow Run ID: {run_id}\")\n",
    "print(f\"\\nTo export this model for deployment, run:\")\n",
    "print(f\"python 2_optional_export_model.py --run-id {run_id} --folder /folder/to/save/model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
